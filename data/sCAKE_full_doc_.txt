Accepted Manuscript
sCAKE: Semantic Connectivity Aware Keyword Extraction
Swagata Duari, Vasudha Bhatnagar
PII: S0020-0255(18)30852-1
DOI: https://doi.org/10.1016/j.ins.2018.10.034
Reference: INS 14015
To appear in: Information Sciences
Received date: 21 December 2017
Revised date: 12 September 2018
Accepted date: 23 October 2018
Please cite this article as: Swagata Duari, Vasudha Bhatnagar, sCAKE: Semantic Connectivity Aware
Keyword Extraction, Information Sciences (2018), doi: https://doi.org/10.1016/j.ins.2018.10.034
This is a PDF file of an unedited manuscript that has been accepted for publication. As a service
to our customers we are providing this early version of the manuscript. The manuscript will undergo
copyediting, typesetting, and review of the resulting proof before it is published in its final form. Please
note that during the production process errors may be discovered which could affect the content, and
all legal disclaimers that apply to the journal pertain.
ACCEPTED MANUSCRIPT
ACCEPTED MANUSCRIPT
sCAKE: Semantic Connectivity Aware Keyword
Extraction
Swagata Duaria,∗
, Vasudha Bhatnagara
aDepartment of Computer Science, University of Delhi, India
Abstract
Keyword Extraction is an important task in several text analysis endeavours.
In this paper, we present a critical discussion of the issues and challenges in
graph-based keyword extraction methods, along with comprehensive empirical
analysis. We propose a parameterless method for constructing graph of text that
captures the contextual relation between words. A novel word scoring method is
also proposed based on the connection between concepts. We demonstrate that
both proposals are individually superior to those followed by the sate-of-theart graph-based keyword extraction algorithms. Combination of the proposed
graph construction and scoring methods leads to a novel, parameterless keyword
extraction method (sCAKE) based on semantic connectivity of words in the
document.
Motivated by limited availability of NLP tools for several languages, we also
design and present a language-agnostic keyword extraction (LAKE) method.
We eliminate the need of NLP tools by using a statistical filter to identify
candidate keywords before constructing the graph. We show that the resulting
method is a competent solution for extracting keywords from documents of
languages lacking sophisticated NLP support.
Keywords: Automatic Keyword Extraction, Text Graph, Semantic
Connectivity, Parameterless, Language Agnostic
∗Corresponding author
Email address: sduari@cs.du.ac.in (Swagata Duari)
Preprint submitted to Journal of Information Sciences October 23, 2018
ACCEPTED MANUSCRIPT
ACCEPTED MANUSCRIPT
1. Introduction
Modern search engines and document databases are tasked with identifying
and locating information with high efficiency. This is typically done using keywords - a small set of relevant and important terms that sufficiently describe
5 the given document. Keyword extraction task is associated with extracting such
terms from a document. According to Ohsawa et al. [31], assigning representative terms to a document is a process called indexing and the terms assigned are
known as keywords. Indexing significantly reduces the human effort in sifting
through vast amounts of information. With monotonically growing reposito10 ries of digital documents, study of automatic keyword extraction methods has
attracted serious attention [5, 7, 8, 13, 19, 25, 29, 30, 32, 33, 44]. Effective keyword extraction methods lead to improved indexing in massive text repositories,
thereby enhancing the quality of retrieved search results.
Automatic keyphrase extraction is a natural extension of keyword extraction
15 problem, where instead of only unigrams, phrases (n-grams) are identified as
potentially relevant descriptors of a document. Mihalcea et al. suggest that
keyphrases can be constructed from keywords as post-processing step by collapsing co-occurring candidates into phrases [30]. The phrases are then ranked
by averaging the scores of the individual terms contained in it. The primary
20 task still remains efficiently extracting quality keywords from the documents,
which is why we focus on automatic keyword extraction problem.
Earliest works on automatic keyword extraction employed purely statistical
techniques based on term frequency to gauge importance of the words [27, 37].
Harter [16] and Bookstein et. al. [3] explored probabilistic approaches for auto25 matic keyword indexing using 2-Poisson distribution model to represent specialty
words. According to another hypothesis, keywords follow a non-homogeneous
distribution and tend to form clusters [32, 48]. In recent years two lines of
development of keyword extraction methods have gained prominence. First of
these is the machine learning based approaches and the second is based on the
30 graph representation of text.
2
ACCEPTED MANUSCRIPT
ACCEPTED MANUSCRIPT
Machine learning approaches come in supervised [19, 39, 42] and unsupervised [25, 29, 30] flavors. Supervised learning methods require labelled training
data to induce the model. Each instance in the training set represents a term in
the document with label 1 (keyword) or 0 (not a keyword). Creation of training
35 set requires manual annotation of the text, making the task tedious, subjective,
and possibly inconsistent. Because of the intense human intervention required,
supervised methods for keyword extraction have not been able to sustain interest and popularity. Due to this reason, unsupervised methods are favored as
alternative approach for identifying keywords.
40 Graph-based approaches denote candidate keywords as nodes and the relationship between two nodes as an edge. Different types of scoring functions are
used to rank the candidates based on specific graph property, e.g., centrality
measure [13, 24, 25, 30], k-degeneracy [33, 38], etc. Performance of graphbased approaches is influenced by the pre-processing steps, graph construction
45 method, and nature of the scoring function.
Existing state-of-the-art graph-based keyword extraction methods suffer from
three limitations. First, the methods require user parameters during graph construction and word scoring stages [30, 33, 38], which cast the burden of careful
tuning of the parameters on the user. Second, the scoring methods rely only on
50 co-occurrence relation between the candidate keywords, while completely ignoring semantic relationship. Finally, these methods use linguistic tools to filter
candidates from the document, limiting their use for many tool-poor languages.
These observations motivate - (i) design of parameterless graph-based method for
improving usability; (ii) design of word scoring methods that account for seman55 tic connectivity among the words, and (iii) development of language-independent
keyword extraction methods. Research in these directions is quintessential for
advancing the state-of-the-art.
1.1. Our contribution
In this paper we present an in-depth study of current state-of-the-art graph60 based keyword extraction methods. We advance the state-of-the-art by propos3
ACCEPTED MANUSCRIPT
ACCEPTED MANUSCRIPT
ing two algorithms for automatic keyword extraction - one for languages with
support of sophisticated NLP tools, and the other for languages that lack support of NLP tools, e.g., Indian languages. Specifically, our contributions are:
i critical discussion of the issues and challenges of graph-based keyword
65 extraction methods (Section 4).
ii design of a novel, parameterless method for constructing a context-aware
graph of text (Section 5).
iii design of a novel word scoring method that aims to capture (i) contextual hierarchy, (ii) semantic connectivity, and (iii) positional weight of the
70 words in the text (Section 6).
iv experimental evaluation of items (ii) and (iii) individually, and comparison
with counterparts in state-of-the-art methods (Sections 5.2 and 6.6).
v design of a novel parameterless, semantic Connectivity Aware Keyword
Extraction method (sCAKE) by integrating (ii) and (iii), and its perfor75 mance evaluation (Section 7).
vi design of Language Agnostic Keyword Extraction method (LAKE) to extend keyword extraction service to languages that lack support of sophisticated NLP tools (Section 8).
We review existing literature in Section 2, followed by experimental setup
80 and dataset details in Section 3. Please note that we are compelled to place
experimental setup early in the paper because of our intention to investigate,
both individually and together, the graph construction and word scoring methods in the state-of-the-art. Section 9 concludes the paper. We apologize for
disappointing the reader who is looking for an explicit section on performance
85 evaluation.
2. Related works
Works related to automatic keyword extraction methods emanate from largely
four approaches. Statistics-based approaches use simple and intuitive statistics
4
ACCEPTED MANUSCRIPT
ACCEPTED MANUSCRIPT
like frequency [27, 37] and spatial distribution of terms [3, 17, 18, 32, 48] to
90 identify candidate keywords. Linguistic approaches for identifying keywords use
some form of linguistic analysis including lexical, semantic, and discourse analysis [10, 11, 19, 34]. Machine Learning approaches (supervised and unsupervised)
have found immense popularity in recent years, which involves training a model
for identifying keywords from texts [5, 14, 19, 25, 29, 30, 39, 42, 46]. Graph-based
95 approaches represent the text as graph, where nodes denote unique terms and
edges define the relationship among nodes. Candidate terms are ranked using
either local or global graph properties [13, 25, 29, 30, 31, 33].
Since statistic- and graph-based approaches are closely related to our work,
we review selected research works from these areas in the following subsections.
100 2.1. Statistics-based Methods
Statistical methods are the earliest keyword extraction techniques. The primary objective of early methods was to solve the problem of automatic indexing
using term frequency [27, 37]. Luhn introduced Term Frequency (TF) to measure the extent of relevance of the words in a text document [27], which was later
105 improved by introducing Inverse Document Frequency (IDF) [37]. Words with
high TF-IDF scores are considered important, and are used for indexing. One
major limitation of TF-IDF method is its being corpus dependent, which restricts its applicability to dynamic collections. Later, Harter [16] and Bookstein
et al. [3] explored the use of 2-Poisson distribution model to identify relevant
110 terms in the document. Harter introduced a measure of indexability to reflect
the relative significance of words in a document [17].
According to another hypothesis, keywords tend to exhibit high degree of
self-attraction leading to non-homogeneous distribution that manifests as clusters [32, 48]. Ortuno et al. conjectured that the standard deviation of positions
115 of occurrence of a word w indicates its degree of relevance in the document,
with higher values interpreted as higher degree of relevance [32]. Zhou and
Slater advanced this idea and proposed two measures - σ-index and Γ-index to
quantify relevance of words in text [48]. Computation of σ-index is similar to
5
ACCEPTED MANUSCRIPT
ACCEPTED MANUSCRIPT
the approach proposed in [32], with minor modifications in the boundary con120 ditions. Both Γ-index and σ-index exploit the spatial distribution of the words
in the text document. Herrera et al. [18] proposed an index for keyword extraction based on Shannon’s entropy. Carratero et al. empirically showed that the
entropy-based methods are sensitive to the choice of partition [8], which is an
undesirable property.
125 2.2. Graph-based Methods
With words in the text represented as nodes, and relationship among them
represented as edges, graph of text proved to be a rich and popular data model
for analyzing text [13, 25, 29, 30, 31, 33, 38]. Blanco et al. describe different
types of edge relationships that can be established among the nodes in a graph
130 of text [2]. Term co-occurrence is the most commonly used relation, where
the graph is constructed by linking the terms co-occurring within a window
of pre-specified size. Subsequently, a word scoring mechanism that exploits
discriminating properties of nodes is used to identify keywords.
KeyGraph method proposed by Ohsawa et al. segments the co-occurrence
135 graph into clusters [31], where each cluster corresponds to a concept. The terms
in each cluster are ranked using a probability-based measure that quantifies
the relationship of each term to the parent cluster, and top ranking terms are
extracted as keywords. Mutsuo et al. established that co-occurrence text graphs
exhibit ‘small-world’ property [29]. They proposed KeyWorld scoring method
140 based on the contribution of each node of the graph to the small world property.
TextRank [30] is the most popular graph-based keyword extraction method so
far. The method scores a node using PageRank [6] algorithm, which takes into
account the global topology of the text graph. Litvak and Last proposed a degree
based keyword extractor, DegExt, which exploits degree property of nodes and
145 is computationally more efficient than TextRank [25]. PositionRank [13] is an
extension of TextRank that takes into account the positional information of
terms in the document to assign weights to the candidate keywords, favoring
words occurring towards the beginning of the text. This method reaffirms the
6
ACCEPTED MANUSCRIPT
ACCEPTED MANUSCRIPT
positional importance of the words accorded by statistical methods.
150 Rousseau et al. hypothesized that the nodes participating in the most cohesive connected component of the text graph are apt candidates for keywords [33].
They performed core-based decomposition [36] of the graph to obtain the keywords. On a similar note, Tixier et al. [38] performed truss-based decomposition [9] to retain words from the top-truss as keywords. These methods are
155 parameter-free since the number of keywords extracted by these methods adapt
to the structure of the graph.
We build over several of these ideas, including hierarchy used in [33, 38],
concepts in text [31], and importance of position of the word proposed in [13],
and propose a parameterless keyword extraction algorithm.
160 3. Experimental Setup
We use R (version 3.3.1) and Python (version 2.7.12) for implementation1
,
using functions from NLP, igraph, openNLP, tm, foreach, and doSNOW packages2
. We execute the programs on a 64-bit PC with 8GB RAM, and Intel Core
i7-6700 CPU @ 3.40GHz 8-core Processor running Ubuntu 16.04 LTS.
165 We use four benchmark datasets shown in Table 1 for empirical observations and comparisons. These datasets have been used extensively to evaluate
keyword extraction algorithms [4, 19, 30, 33, 35, 43]. Table 1 presents general
properties of the four datasets, including number of documents in corpus, average document length, average number of gold-standard keywords along with
170 standard deviation, and average percentage of candidate keywords. Hulth2003
documents, which are abstracts, are the shortest. Krapivin2009 documents have
least average number of keywords assigned to them. It is noteworthy that the
average number of candidates lies in the range of 40-45% of the document length.
For evaluation, we use the uncontrolled list of keywords for Hulth2003,
175 gold-standard keywords for Krapivin2009 and NLM500, and author-and-reader1The code for implementation is available at https://github.com/SDuari/sCAKE-and-LAKE
2https://cran.r-project.org/web/packages/
7
ACCEPTED MANUSCRIPT
ACCEPTED MANUSCRIPT
Table 1: Overview of experimental datasets. |D|: Number of documents in corpus, L: average
document length, Avg/sd: average number of gold-standard keywords per document/standard
deviation, C: average percentage of candidate keywords (nouns and adjectives)
Dataset |D| L Avg/sd C Dataset Description
Hulth2003*
[19] 1500 129 23/12.44 45.97 Abstracts from Inspec database
NLM500 [1] 500 4854 27/10.38 44.08 Full papers from PubMed database
Krapivin2009 [23] 2304 7961 11/6.44 40.5 ACM full papers
SemEval2010*
[22] 244 8085 34/10.35 40.05 ACM Digital Library papers
* We use Test and Training Sets.
assigned keywords for Semeval2010. We use classical F1-measure to evaluate
performance of the compared algorithms for top-k extracted keywords. The
results are macro-averaged at the dataset level. We consider TextRank [30],
DegExt [25], k-core retention [33], and PositionRank [13] as our prime competi180 tors and evaluate the proposed approaches against them.
For each dataset, we experimented with all algorithms to find the value of k
that yields the best F1-measure. It was observed that the highest F1-measure
was obtained for k = 25 for Hulth2003, k = 10 for Krapivin2009, and k = 30
for NLM500 and SemEval2010 datasets. We use these values of k for reporting
185 results for corresponding datasets in subsequent experiments. It is pertinent
to note that the values correlate with the average number of gold-standard
unigrams (Column 4 of Table 1) annotated for the datasets.
4. Graph-based Keyword Extraction: Issues and Challenges
Graph-based keyword extraction algorithms perform three generic steps in
190 sequence - (i) pre-processing of text to identify candidate keywords, (ii) transforming text to graph with candidates as nodes, and (iii) scoring the candidates
based on some local or global graph property. Figure 1 depicts the process of
graph-based automatic keyword extraction. It is the variation in design of the
core steps and their execution that produces a bouquet of graph-based keyword
195 extraction algorithms [12, 13, 15, 25, 26, 29, 30, 31, 33]. In the following subsections, we discuss the variations of these three steps and deliberate on the issues
8
ACCEPTED MANUSCRIPT
ACCEPTED MANUSCRIPT
and challenges faced by graph-based keyword extraction approaches. Each subsection focuses on one task, delineates the challenges, and describes how the
challenges are addressed by the existing algorithms. We support our arguments
with empirical evidences, wherever relevant.
Figure 1: Sequence
of sub-tasks in graphbased keyword extraction methods
200
4.1. Pre-processing of Text
Pre-processing of text significantly affects the resulting keywords because
the output from this step is the primary input to the graph construction phase.
A different combination of pre-processing sub-steps has a defining effect on performance of the methods. Tokenization and stopword3
205 removal are performed
by all algorithms [13, 25, 30, 33]. Barring DegExt, all algorithms perform POS
tagging and agree that nouns and adjectives are the prime candidates for keywords [13, 30, 33]. DegExt doesn’t inflict any restriction over the candidates except for stopwords, which are similarly disregarded in all methods. Only k-core
210 retention algorithm [33] uses stemming, and claims that it boosts performance.
Average recall for any algorithm for a particular document is bounded by
the percentage of gold-standard keywords actually present in the document.
We studied the gold-standard keyword lists of the four datasets and found that
stemming increases the upper bound for recall in all datasets. First column in
215 Table 2 shows this bound without stemming the documents, and the second
column shows the bound after stemming.
3Frequently used words, called stopwords, are disregarded during automatic keyword extraction.
9
ACCEPTED MANUSCRIPT
ACCEPTED MANUSCRIPT
Dataset w/o stemming With stemming
Hulth2003 89.86013 92.0831
NLM500 70.58481 79.2508
Krapivin2009 96.88258 98.17081
SemEval2010 95.91513 98.95135
Table 2: Percentage of gold-standard
keywords present in text with and
without stemming
The issue of effective sequence of pre-processing steps for keyword extraction
is more or less settled. However, a vast majority of languages fail to benefit
from existing keyword extraction methods due to the lack of sophisticated NLP
220 tools required for pre-processing by these methods. We address this issue later
in Section 8.
4.2. Graph Construction
Existing keyword extraction algorithms exhibit wide variations in the process
of constructing graph from text. The resulting structural differences naturally
225 cascade into differential in graph properties. Since graphs are principal inputs
for ranking the candidates, the word scores and the set of extracted keywords
veritably differs for different algorithms.
Variations in graph construction methods align primarily in two dimensions.
First, the set of candidate keywords obtained after pre-processing the text. This
impacts the order4
230 of the graph and its properties. For example, candidate lists
produced after stemming creates a smaller graph as compared to those produced
without stemming. Second is the scheme for defining relationship between the
nodes (i.e. the edge set), which affects the construction and size5 of text graph.
Edge direction and edge weight are other considerations for graph construction.
235 DegExt [25] constructs unweighted, directed graph corresponding to the order
of words in original text. Other methods construct weighted, undirected graph
of text where edge weight is the frequency of co-occurrence of the two words.
Variations in the text graphs are more conspicuous because of the second
dimension. Two parameters, viz. window-size and source text for sliding the
4Order of a graph is the cardinality of the node set.
5Size of a graph is the cardinality of the edge set.
10
ACCEPTED MANUSCRIPT
ACCEPTED MANUSCRIPT
240 window emerge as fundamental causes of differences in edge sets and the resultant graphs. Though all existing algorithms use co-occurrence of words within
a specified window as the relationship, it is the size of the window that induces
pronounced differences. Different keyword extraction methods recommend different window sizes. TextRank suggests window size of 2-10 and compares 2,
245 3, 5, and 10 for experimental evaluation (Page 5, Table 1 of [30]). DegExt uses
window of size 2 that does not connect words separated by punctuation marks
(Page 3, [25]), while k-core retention algorithm uses window of size 4 (Page 4,
[33]). Apparently, the choice of window size parameter in all works is based on
empirical observation over the experimented datasets.
250 Differences in the text graphs are further accentuated by the source text
where the relationship is examined. Some methods recommend sliding the window on raw text [13, 30], while others slide on pre-processed text [25, 33]. There
is no systematic and scientific study of these two parameters (window size and
source text) of graph construction methods to the best of authors’ knowledge.
255 Lack of consensus on these two issues poses difficult decision choices for the
users and the designers of the algorithm.
Graph
Graph construction
w Directed Weighted? Source Overspan
TG 2 to 10 No Yes Original Yes
GoW 4 No Yes Processed Yes
DG 2 Yes No Processed No
Table 3: Comparison of graphconstruction methods. w: window size parameter, Source:
text to slide window, Overspan: connect words separated
by punctuation marks.
Table 3 summarizes the differences in graph construction approaches adopted
by the state-of-the-art keyword extraction methods. In this table (and all others
following), we use acronyms for graphs created by TextRank and PositionRank6
260 (TG), DegExt (DG), and k-core retention (GoW) algorithms. Figure 2 shows
the graphs constructed by three different algorithms (2b, 2b, and 2d) for the
6PositionRank uses same settings as TextRank for pre-processing and graph construction.
11
ACCEPTED MANUSCRIPT
ACCEPTED MANUSCRIPT
same sample text (2a), highlighting the differences among the graph construction approaches.
(a) Text document
(b) Graph-of-Word (GoW) [33]
(c) TextRank Graph (TG) [13, 30].
12
ACCEPTED MANUSCRIPT
ACCEPTED MANUSCRIPT
(d) DegExt Graph (DG) [25]
Figure 2: Text graphs created by different algorithms for document id 2015 of Hulth2003 Test
dataset. Edge width is proportional to the corresponding edge weight. TG and GoW graphs
are constructed with window-size 4 and DG graph with window-size 2.
4.3. Word Scoring
265 Word scoring methods are crucial discriminators between keyword extraction
algorithms. TextRank [30] uses PageRank [6] algorithm to assign importance
to candidates by recursively taking into account importance of its neighbors.
Thus, the knowledge drawn from the global graph structure is used to rank the
words. TextRank uses a parameter called damping factor7 d, which is set to
270 0.85 following [6]. We examine the impact of damping factor on performance
of the algorithm. Our experiments on Hulth2003 dataset (used for evaluation
by TextRank) reveal that best performance is achieved for different values of
damping factor for different window sizes. Specifically, the best result in terms
of F1-score is obtained for window-sizes 2, 3, and 4 when d is set to 0.85, 0.9,
275 and 0.95, respectively. Among different combinations of the two parameters,
window-size 4 and d = 0.95 yields best result. This is purely an empirical
observation specifically for this dataset. We are not in position to ascribe any
7Damping factor is associated with the concept of random jump in web search.
13
ACCEPTED MANUSCRIPT
ACCEPTED MANUSCRIPT
theoretical reason to the phenomenon, but state this to highlight the sensitivity
of the end results towards the algorithmic parameters. We use window-size 4
280 and d = 0.95 for subsequent experiments in accordance with our observation.
DegExt [25] uses degree centrality to score the relevance of candidates. Authors claim to achieve performance comparable to TextRank with lesser computational complexity. K-core retention algorithm [33] doesn’t score candidates
explicitly. Instead, it uses core decomposition of the weighted graph and retains
285 words from the top core as keywords. PositionRank [13] uses position-biased
PageRank to rank the candidates by favoring words that occur towards the beginning of the text, and uses same parameters as TextRank. Table 4 summarizes
the word-scoring methods and their respective parameters for four methods.
KE
Algorithms
Word Scoring
Scoring Method Sparams Value
TextRank PageRank
d 0.85
t 1e-4
n Top 1/3
K-core
Weighted k-core
decomposition
- -
DegExt Degree Centrality n U
PositionRank
Position-biased
PageRank
d 0.85
t 1e-3
n U
Table 4: Comparison of word scoring
methods for different Keyword Extraction algorithms. Sparams: Algorithmic parameters for word scoring
method as used by published works,
d: damping factor, t: convergence
threshold, n: Number of keywords to
be extracted, U: User parameter.
To the best of authors’ knowledge, investigation of the combination of pre290 processing and graph construction methods that yields best performance for keyword extraction methods is pending.
4.4. Evaluation of Keyword Extraction Methods
No system is capable of definitive assessment of relevance of the words in a
document because relevance is subjective not only with respect to the reader of
295 the document, but also with respect to time. Further, evaluation of keyword
extraction method is based on the assumption that importance of words is a
dichotomous variable that is user-specific, and is defined outside the system.
It is therefore imperative to evaluate keyword extraction methods against a
14
ACCEPTED MANUSCRIPT
ACCEPTED MANUSCRIPT
gold-standard keywords list.
300 Automatic keyword extractors are judged on the basis of how precisely they
extract and how well they recall the keywords that exist in gold-standard keywords list [13, 19, 25, 30, 33, 42]. Gold-standard keywords are manually annotated, and hence often subjective and noisy. Over- and under-annotation
in gold-standard lists influence the performance of keyword extractors. Conse305 quently, performance of one algorithm may be different for different datasets.
Recently, Florescu et al. [13] used mean reciprocal rank (MRR) to evaluate
the performance of their algorithm, which is based on the single highest-ranked
relevant item. However, we believe that MRR is better suited for evaluation of
web search methods where the single highest-ranked relevant item is important
310 for the user. Since number of keywords required is more than one, MRR could
be misleading for evaluating performance of automatic keyword extractors.
Most algorithms accept the number of keywords to be extracted as a user
parameter [13, 25, 39] or a pre-decided value [30]. Alternatively, this number can
be set to take a value based on the structure of the text graph [33, 38]. Higher
315 value of this parameter is often associated with higher recall and low precision,
while lower value is associated with lower recall and high precision [28]. Algorithms may choose to match either unigrams [33, 38] or keyphrases [13, 30]
against the gold standard list. However, there is no consensus in literature regarding evaluation of keyphrase extraction approaches, as it is not clear whether
320 to reward or penalize a method that over- or under-estimates keyphrases given
the gold-standard list [33].
Performance of keyword extraction methods varies depending on the parameter settings used, as well as the properties of experimental datasets. No algorithm
is able to perform uniformly well across domains and corpora.
325 5. Context-aware Graph Construction Method
Motivated by the desideratum to design parameterless graph construction
method, we propose to construct co-occurrence graph based on pragmatics of
15
ACCEPTED MANUSCRIPT
ACCEPTED MANUSCRIPT
written communication. Unlike semantics, which studies the meaning coded
in the language, pragmatics involves study of transmission of meaning depend330 ing on the context of utterance. The context set by a sentence is often used
by the consecutive sentences, imparting continuity in communication. This
phenomenon, called entailment, is a well studied concept in linguistics. Transmission of context from one sentence to another is the core idea underlying the
proposed co-occurrence graph construction method.
335 In this method, the window slides over two consecutive sentences and the
candidates co-occurring therein are linked. This eliminates the need of integervalued window-size parameter, and captures contextual co-occurrence of words8
(terms) in text. The resulting graph, called Context-Aware Text Graph (CAG),
is formally represented as GCAG = (V, E, W). Here, V is the set of nodes
340 representing the candidate words, E is the set of edges (co-occurrence relation),
and W is the set of corresponding edge weights. Weight wij for an edge eij
indicates the co-occurrence frequency of two words vi and vj in the text. Higher
value of wij indicates stronger contextual relationship between words vi and vj .
For graph creation, we consider two consecutive sentences (sk and sk+1)
in the given text as one document (dk), and create a Boolean term-document
matrix C, where
cik =



1, if term ti occurs in dk
0, otherwise
In accordance with the convention, we use the set of nouns and adjectives
as candidates to construct matrix C. Let T = CCT 345 denote the term-term matrix where τij represents the number of co-occurrences of terms ti and tj in
the documents (pairs of consecutive sentences). Note that T is the symmetric adjacency matrix of an undirected, weighted graph G. The context-aware
text graph, GCAG(V, E, W), is constructed from T after zeroing the diagonal
350 elements. Figure 3 shows a sample graph created using the proposed CAG
8We use ‘word’, ‘node’ and ‘term’ interchangeably in the rest of the paper.
16
ACCEPTED MANUSCRIPT
ACCEPTED MANUSCRIPT
method for the text shown in Figure 2a. We observe that the graph created by
CAG method is denser than those in Figure 2. This is because of the bigger
co-occurrence span (two consecutive sentences) used in CAG method.
Figure 3: Context-Aware
Text Graph for the sample
text shown in Figure 2a.
5.1. Comparison of Graph Properties
355 We analyze the structural properties of the TG, DG, GoW, and CAG graphs
for the four datasets mentioned in Section 3. We construct four types of graphs
for each document in the datasets, and compute number of nodes and edges,
global clustering coefficient9
[45, p 101], average path length [45, p 98], and
density [41, p 101]. Tables (5a-5d) show the variations in topological properties
360 of graphs by averaging the results at the dataset level.
Even though the four algorithms consider nouns and adjectives as candidates, average number of vertices in each graph type differs depending on the
nature of edge connections. The variation in edge relation resulting due to different window size yields distinct sets of isolated vertices, which when excluded
365 from the graphs results in different node sets. Some observations about CAG
graphs are - (i) number of nodes is minimum in CAG method for all datasets, (ii)
number of edges is highest for CAG because the co-occurrence span is usually
larger than the window sizes adopted in other methods, and (iii) CAG graphs
are denser than other graphs. The number of edges created per window slide
9Also called transitivity of graph G.
17
ACCEPTED MANUSCRIPT
ACCEPTED MANUSCRIPT
Table 5: Topological properties of graphs constructed using TextRank (TG), DegExt (DG),
k-core (GoW), and CAG for four dataset. |V |: number of nodes, |E|: number of edges, CC:
global clustering coefficient, AP L: average path length, ∆: Density
(a) Hulth2003 dataset.
Method |V| |E| CC APL ∆
TG 39 67 0.40 3.37 0.11
DG 37 37 0.05 3.91 0.033
GoW 35 143 0.49 2.10 0.27
CAG 33 370 0.85 1.30 0.70
(b) Krapivin2009 dataset.
Method |V| |E| CC APL ∆
TG 716 2930 0.15 3.32 0.012
DG 697 1636 0.078 5.11 0.004
GoW 555 5022 0.21 2.57 0.035
CAG 471 19664 0.51 1.87 0.16
(c) NLM500 dataset.
Method |V| |E| CC APL ∆
TG 589 2083 0.17 3.62 0.013
DG 540 938 0.06 6.04 0.004
GoW 479 3647 0.22 2.67 0.036
CAG 397 11514 0.44 1.90 0.151
(d) Semeval2010 dataset.
Method |V| |E| CC APL ∆
TG 770 3085 0.15 3.35 0.012
DG 727 1528 0.071 5.32 0.003
GoW 617 5385 0.20 2.63 0.029
CAG 507 13441 0.38 1.99 0.105
370 depends on the number of candidates present within the co-occurrence span.
Maximum number of edges created each time the integer-valued window of size
w slides is (w − 1), whereas for CAG it is bounded by (|Si
| + |Si+1| − 1), where
|S| is the number of words in the sentence. This makes the CAG graph denser
than the other algorithms.
375 Due to the dense nature of CAG graphs, clustering coefficient is higher and
average path length is lower for CAG. Other graphs are visibly less dense as
compared to CAG graphs (Figures 2 and 3). DG graphs are the most sparse
among these four types and thus have lowest clustering coefficient and highest
average path length. This is due to the fact that DG uses a window-size of 2 as
380 co-occurrence span for connecting nodes, which results in nodes being connected
to a fewer nodes than the other three methods. Variations in the structural
properties of graphs play an instrumental role in word scoring, as discussed in
the following subsection.
5.2. Performance Evaluation of CAG
385 We compare effectiveness of the four state-of-the-art graph construction
methods with the proposed CAG method by applying native word scoring meth18
ACCEPTED MANUSCRIPT
ACCEPTED MANUSCRIPT
ods on the respective graphs, as well as on the context-aware graphs. Following
the approach adopted by Rousseau et al. [33], we match k keywords (as defined
in Section 3) extracted from each document against the gold-standard keywords
390 (as unigrams) to compute the performance evaluation metrics. Table 6 shows
the experimental results as macro-averaged F1-score.
Table 6: Comparative Evaluation of original vs. CAG graphs for native scoring methods in
terms of macro-averaged F1-score. PageRank, Degree, k-core, bised PageRank: word scoring
methods for TextRank, DegExt, K-core, and PositionRank respectively
Word Scoring
Methods
Graph
Datasets
Hulth2003 Krapivin2009 NLM500 Semeval2010
PageRank
Original 18.37 13.72 10.73 13.65
CAG 49.54 35.05 25.68 41.54
Degree
Original 18.22 13.34 10.91 14.36
CAG 49.42 34.92 25.59 40.81
k-core
Original 43.41 22.70 20.20 29.34
CAG 34.84 3.46 2.12 3.60
biased PageRank
Original 50.41 37.07 21.94 27.50
CAG 51.01 42.86 27.54 35.80
We observe that CAG graphs significantly boost F1-score of all scoring methods except k-core retention. Applying k-core decomposition on CAG graphs
results in fewer nodes at the top core. This decreases recall significantly even
395 though the precision is high, leading to a drastic drop in F1-score. We also
note that PositionRank outperforms TextRank, K-core retention, and DegExt
when applied on CAG graphs. This experiment establishes the effectiveness
of context-aware graph construction method.This also affirms that capturing
the context in the window that spans two consecutive sentences highlights the
400 important words irrespective of the scoring method used.
5.3. Timing Comparison for Graph Construction
In order to gauge the computational efficiency, we compare the time taken by
the four graph construction methods (including pre-processing). Table 7 shows
average time required per document to construct text graphs of four datasets.
405 The timings (in seconds) are averaged over three executions for each data set.
19
ACCEPTED MANUSCRIPT
ACCEPTED MANUSCRIPT
Table 7: Average time (in seconds) taken per document by the four algorithms on each dataset.
Best timings are presented in bold.
Methods
Datasets
Hulth2003 Krapivin2009 NLM500 Semeval2010
TG 0.5245 35.30 30.94 52.84
DG 0.3937 20.88 8.022 12.78
GoW 0.0859 16.88 20.97 43.71
CAG 0.079 3.080 1.895 3.412
CAG method is found to execute significantly faster than other three methods on all datasets. It is important to note that the number of sentences is
much less than the number of distinct windows of size w. For a document of
length N consisting of S sentences (N  S), the co-occurrence identification in
410 sliding window based algorithms is processed (N − w) times, while in CAG it is
processed (S − 1) times. This explains the speedy execution of CAG method.
6. Semantic Connectivity based Word Scoring Method
We exploit semantic connectivity between words in a document to identify
important and relevant words, and propose a novel word scoring method. The
415 proposed method leverages - (i) the level of hierarchy of a word in text graph,
(ii) its semantic relationship with neighbors, (iii) the extent of its semantic
connectivity, and (iv) its positions of occurrence in the text. It is pertinent to
note that we do not use any linguistic tool to capture semantic aspects.
6.1. Level of Hierarchy
420 Recently, it has been established that hierarchy of nodes (words) in cooccurrence graphs is the sole determinant of the importance of the word [33,
38]. Rousseau et al. [33] used core-based decomposition and Tixier et al. [38]
used truss-based decomposition to obtain the hierarchy. Though efficient, these
methods have two major limitations. First, not only the keywords but even the
425 number of keywords in the text is determined singularly by the hierarchy. This
20
ACCEPTED MANUSCRIPT
ACCEPTED MANUSCRIPT
may result in too few keywords, thereby degrading the recall. Second, both
decomposition methods have low semantic interpretability when used singly.
Subscribing to the view advanced by Tixier et al., which shows truss-based
decomposition works better than core-based decomposition, we use trussness [9]
430 to elicit hierarchy of words in the graph. Truss-based decomposition is a graph
peeling algorithm that results into a sequence of subgraphs (called trusses), each
of which is denser than the previous one. Cohen observed and we quote “The
k-truss provides a nice compromise between the too-promiscuous (k − 1)-core
and the too-strict clique of order k” [9]. We briefly introduce k-truss and the
435 concept of trussness below, adapting definition from [9].
Definition 6.1. For a weighted, undirected, simple graph G = (V, E, W), a
k-truss subgraph of G is the maximal subgraph, Gk = (Vk, Ek, Wk), such that
each edge eij ∈ Ek belongs to at least (k − 2) triangles.
Thus, truss based decomposition results in a hierarchy of subgraphs with
440 G itself being a 2-truss graph. Gi+1 (at level (i + 1)) is a subgraph of Gi (at
level (i)). An edge eij is said to be at trussness level lij = k if it lies in ktruss but not in (k + 1)-truss. Higher truss level of an edge eij indicates its
participation in more triangles and hence, more number of common neighbors
for nodes vi and vj . An example graph G and its k-trusses are shown in Figure
445 4. In this example (Figure 4b), darker colors indicate higher truss level of the
edges. Graph G is decomposed into 3 subgraphs - 2-truss (graph G itself), 3-
truss (graph G excluding light grey edges), and 4-truss (only dark grey edges).
Hierarchy of edges naturally translates to the hierarchy of nodes linked by
450 the edges. Extending the concept of trussness to nodes, Kaur et al. [21] define
truss level λi of node vi as follows.
Definition 6.2. Truss level λi of node vi
is defined as
λi = maxvj∈Ni {lij} (1)
where Ni
is the set of neighbors of node vi and lij is the truss level of edge eij .
21
ACCEPTED MANUSCRIPT
ACCEPTED MANUSCRIPT
(a) Graph G (b) k-truss subgraphs of G
Figure 4: Truss-based decomposition of graph G
Higher truss level of a node is the evidence of greater extent of its connectivity to other nodes. Figure 5 shows truss-based decomposition and the cor455 responding node truss levels for the sample graph in Figure 3. Different colors
indicate different truss levels, with darker colors representing higher truss level
of the nodes. The graph is decomposed into 4 subgraphs10 - 9-truss (the graph
itself), 12-truss (the graph G excluding light grey nodes), 16-truss (graph G
induced by two darker shades), and 22-truss (graph induced by darkest shade).
460 In the context of text graph, existence of a node at a particular truss level
indicates the hierarchy level at which the word (node) vi
is embedded in the
text. Thus the truss levels of the nodes depict contextual hierarchy of the words
in text. SC-based scoring method recognizes truss level λi of a node as a factor
that determines the importance of the word.
465 To determine the k-truss subgraphs of G, a naive algorithm iteratively removes those edges which are not part of (k − 2) triangles (Please see Cohen [9]
for details). The algorithm has a polynomial time complexity and is bounded
10All intermediate k-truss subgraphs are same. For example, in Figure 5 10-, 11-, and
12-truss subgraphs are same.
22
ACCEPTED MANUSCRIPT
ACCEPTED MANUSCRIPT
Figure 5: Truss-based decomposition of CAG graph
in Fig. 3
above by (nm2 + n), where n is the number of vertices and m is the number of
edges in G [9]. Wang et al. proposed an algorithm for in-memory truss-based
decomposition of the graph, which has time complexity O(m1.5
470 ) and space complexity O(n + m) [40]. We implement this algorithm to perform truss-based
decomposition of context-aware graphs in our experiments.
6.2. Semantic Strength of a Word
Importance of a word in the text is a function of (i) the strength of its
475 semantic relationship with other words co-occurring in the same context, and
(ii) the level of these words in contextual hierarchy. Strength of relationship
between two words vi and vj is marked by the number of times the two words cooccur in same context, and is captured by weight wij of edge eij . The semantic
strength of a word (node) is defined as follows.
Definition 6.3. For a node vi with neighborhood Ni
in graph G, the semantic
strength of vi
is defined as
χi =
X
vj∈Ni
wij × λj (2)
where wij is the weight of edge eij and λj is the truss-level of vj ∈ Ni
480 .
According to Equation 2, semantic strength of word w is the additive function of the co-occurrence frequency with its neighbors and their respective hier23
ACCEPTED MANUSCRIPT
ACCEPTED MANUSCRIPT
archical levels. A word gains strength when it co-occurs frequently with other
words at higher levels of hierarchy.
485 6.3. Semantic Connectivity
A document comprises multiple concepts that are semantically related. KeyGraph algorithm proposed by Ohsawa et al. finds important terms that hold the
rest of the document together via inter-term connectivity between the concepts
manifesting as clusters in a text graph [31]. We extend this idea to quantify im490 portance of a word by counting the number of concepts in which it participates.
In order to avoid computationally expensive task of graph clustering (O(m2
)
in [31]), we use truss as proxy for cluster (concept). The assumption is reasonable since clusters and trusses both highlight denser regions of the graph.
Thus truss-based decomposition of the text graph yields not only the position of
495 words in the hierarchy, but also the hierarchy of concepts. Experimental results
presented in Section 7.2 validate this assumption.
The extent of semantic connectivity of a word is measured by examining
the number of distinct concepts that it links. If more of its neighbors belong
to different concepts, its removal is likely to leave bigger semantic gap in the
500 document. On the other hand, if all neighbors of a word belong to the same
concept, removal of the word leads to little loss of meaning since the semantic
relation among remaining words in the concept remains more or less intact.
Based on this premise, we approximate semantic connectivity of a word by
examining the set of co-occurring words to ascertain the number of distinct
concepts it links. Semantic Connectivity SCi of node vi
505 is the count of distinct
concepts (hierarchy levels) to which its neighbors belong, normalized by the
highest hierarchy level in the graph. We express this measure as follows.
Definition 6.4. For a node vi ∈ G with neighborhood Ni
, the semantic connectivity index of vi
is defined as
SCi =
|{λk : vk ∈ Ni}|
maxtruss
(3)
where maxtruss is the highest truss (hierarchy) level of G.
24
ACCEPTED MANUSCRIPT
ACCEPTED MANUSCRIPT
Thus, a word connected to more words from different levels of hierarchy
510 (truss) binds together more concepts in the text, and is considered important.
6.4. Positional weight
Previous studies hypothesize that keywords tend to occur at the beginning
of the document [13, 19, 47]. PositionRank [13] is a recent development which
capitalizes on this assumption to identify keywords, and is found to be an improvement over the previous methods. Following this premise, we take the
positional weight of each word into account while computing the word score.
As prescribed by [13], each term ti
is assigned a weight based on the positional
information as follows.
ωi =
Xni
j
1
pj
(4)
where ni
is the frequency of term ti and pj is the j
th position of its occurrence
in the document.
Thus, words occurring towards the beginning of the text documents are
515 considered better candidates for keywords and are assigned higher weight than
those occurring towards the end of the text document.
6.5. Word Score
Overall relevance of a word in the document is a function of the level at which
the word is embedded (λ), semantic strength it derives from its co-occurring
words (χ), extent to which it is linked to the concepts present in the document
(SC), and its positional weight (ω). Assuming that these factors have a multiplicative effect on the relevance of the word in a document, word score of the
candidate keyword (node) vi
is defined as follows.
SCScore(vi) = λi ∗ χi ∗ SCi ∗ ωi (5)
Admittedly, more sophisticated functions for word scoring can be designed
and explored empirically. We choose to go with Eq. 5 because of its computa520 tional efficiency, simplicity and ease of interpretation.
25
ACCEPTED MANUSCRIPT
ACCEPTED MANUSCRIPT
Experimental evaluation establishes effectiveness of the proposed scoring
function. For the example text in Figure 2a, PositionRank is not able to extract words like “logarithmic” and “invariant” as the positional weights of these
words pull down their ranks. On the other hand, sCAKE correctly extracts
525 these words because it takes into account the semantic connectivity among
words. This observation establishes that positional information alone is not
sufficient for weighting relevance score for words. Semantic connectivity plays
an important role in identifying important words in a document.
6.6. Empirical Evaluation of SCScore function
530 We evaluate effectiveness of the SCScore function by applying it on four
types of graphs (TG, DG, GoW, and CAG) and comparing the results using
respective native scoring functions. Table 8 reports F1-scores for each of the
graph types on the four datasets, macro-averaged at the dataset level. For ease
of comparison, we repeat the result of the corresponding native scoring methods
from Table 6.
Table 8: F1-score obtained by applying SCScore on different graph types. TGT R: TextRank
on TG graphs, TGP R: PositionRank on TG graphs
Graph
Hulth2003 Krapivin2009 NML500 Semeval2010
native SCScore native SCScore native SCScore native SCScore
TGT R 18.37 51.14 13.72 38.97 10.73 23.28 13.65 34.93
TGP R 50.41 51.14 37.07 38.97 21.94 23.28 27.50 34.93
DG 18.22 46.55 13.34 21.24 10.91 16.01 14.34 23.07
GoW 43.41 43.06 22.70 30.01 20.20 20.80 29.34 29.05
CAG - 51.09 - 43.52 - 28.29 - 40.14
535
We observe that the performance of SCScore is significantly superior than
the native word scoring methods of the four competing algorithms. We further
conclude that the combination of CAG graphs and SCScore scoring method
outperforms the four state-of-the-art methods .
26
ACCEPTED MANUSCRIPT
ACCEPTED MANUSCRIPT
540 7. sCAKE: semantic Connectivity Aware Keyword Extraction
Having illustrated the superiority of CAG graph construction method and
semantic connectivity based word scoring method individually, we now integrate
the two and propose a novel automatic keyword extraction algorithm named
sCAKE. Three stages of the algorithm are as follows:
545 i Candidate Filtration: Following [33], we identify the candidate keywords
as nouns and adjectives, retained after POS tagging11. This step is followed by stopword removal12 and stemming13 of the retained list. The
stemmed version of the list is considered as candidates, and is passed on
to the next stage along with the stemmed version of the original text.
550 ii Graph Construction: We create Context-Aware Text Graphs (CAG) as
described in Section 5. This approach captures the pragmatics of written
communication and connects words that are closely related to each other
depending on the context of their occurrence. Unlike other methods, the
proposed graph construction method is parameter-free.
555 iii Word Score: We compute word score for the candidates using the proposed
semantic connectivity based word scoring method (SCScore) as presented
in Section 6. This method is based on the intuition that a word derives importance from its neighbors and its own position in the text. The SCScore
method exploits the semantic connectivity between words in a document
560 based on their contextual hierarchy. This method tries to capture the semantic aspects solely on the basis of word-to-word relation without using
any linguistic tools.
The candidates are ranked according to their respective SCScore, and the
user can extract top-k candidates.
11https://opennlp.apache.org/docs/1.8.2/manual/opennlp.html
12http://www.lextek.com/manuals/onix/stopwords2.html
13https://tartarus.org/martin/PorterStemmer/
27
ACCEPTED MANUSCRIPT
ACCEPTED MANUSCRIPT
565 7.1. Illustration of Keyword Extraction by Different Methods
We compare the set of keywords extracted by sCAKE and the four stateof-the-art methods. Table 9 shows keywords extracted by the five methods
against gold-standard keywords for the example text shown in Figure 2a. In this
example, we measure commonality of keywords between gold-standard set and
570 the set extracted by each of the five algorithms using Jaccard Index (JI) [20]. It
is evident that JI for sCAKE is highest among all the methods. K-core performs
poorly on this document because it extracts words belonging to the highest core
irrespective of the number of keywords to be extracted.
Table 9: Comparative lists of top-27 keywords extracted by sCAKE and competing methods
against 27 gold-standard keywords for text in Figure 2a. r: Number of keywords that match
to gold standard, JI = Jaccard Index. Words in italics do not match with gold-standard.
Method Keywords r JI
Goldstandard
optical, recognition, object, scale, invariance, classical, convergent,
correlator, realtime, method, information, deformed, fringe,
patterns, fourier, transform, profilometry, technique, property, mellin,
radial, harmonic, decomposition, logarithmic, filter, invariant, factors
27 -
sCAKE
optic, recognit, object, scale, invari, correl, classic,
converg, method, inform, radial, harmon, deform, fring,
pattern, fourier, profilometri, techniqu, properti, approach,
mellin, decomposit, logarithm, filter, target, interv, factor
24* 0.80
TextRank
scale, invariance, objects, d, radial, threedimensional, harmonic,
fourier, patterns, mellin, method, correlator, results, filter, factors,
convergent, classical, logarithmic, decomposition, deformed,
fringe, profilometry, technique, approaches, experimental,
recognition, invariant
21 0.64
PositionRank
optical, recognition, objects, scale, invariance, classical,
threedimensional, convergent, correlator, method, information,
radial, harmonic, patterns, deformed, fringe, fourier, profilometry,
technique, property, d, filter, mellin, decomposition, approaches,
logarithmic, invariant
23 0.74
K-core classic, method, object, three-dimension 3 0.11
DegExt
scale, d, objects, convergent, harmonic, invariance, radial,
threedimensional, approaches, changes, classical, correlator,
decomposition, fringe, information, invariant, logarithmic, mellin,
method, recognition, deformed, different, experimental,
factors, filter, interval, optical
19 0.54
* The keyword ‘invari’ matches two of the gold-standard keywords ‘invariance’ and ‘invariant’.
28
ACCEPTED MANUSCRIPT
ACCEPTED MANUSCRIPT
7.2. Comparative Evaluation of sCAKE with PositionRank
575 We empirically evaluate and compare sCAKE with PositionRank algorithm.
We choose only PositionRank for comparison with sCAKE because it outperforms other three state-of-the-art methods in the earlier experiments. We extract top k candidates14 as keywords and compute precision, recall, and F1-score
for both methods on the four datasets. The empirical results are reported in
Table 10. Bold-faced values indicate maximum F1-score for each dataset.
Datasets
PositionRank sCAKE
P R F1 P R F1
Hulth2003 45.68 64.45 50.41 45.41 66.81 51.09
Krapivin2009 36.95 40.90 37.07 42.48 48.78 43.52
NLM500 19.69 26.60 21.94 24.88 34.99 28.29
Semeval2010 25.31 31.29 27.50 35.82 47.37 40.14
Table 10: Performance
evaluation of sCAKE
vs. PositionRank
580
We observe that the performance of sCAKE is consistently and significantly
better than PositionRank on all four datasets. The improvement for longer
documents is significantly higher. It is reasonable to conclude that sCAKE
extracts markedly better keywords from documents of varied length compared
585 to the competing method.
8. LAKE: Language-Agnostic Keyword Extraction
Most of the existing automatic keyword extraction algorithms use sophisticated NLP tools, which prohibits their application to texts of languages with
meager NLP support. We mitigate this problem by proposing a language agnos590 tic keyword extractor (LAKE) for eliciting keywords from a document written in
language with deficient set of NLP tools. The method profits from the strength
of statistical and graph based methods, sans the burden of linguistic tools.
Like classical graph-based keyword extraction methods, LAKE is orchestrated in three stages. It is the first stage of candidate filtration that makes
14k refers to the values as mentioned in Section 3
29
ACCEPTED MANUSCRIPT
ACCEPTED MANUSCRIPT
595 LAKE unique and imparts language independence. In the following subsections, we describe the candidate filtration approach for LAKE.
8.1. Candidate keywords selection
Unlike existing graph based keyword extraction methods that accept nouns
and adjectives as candidate keywords, LAKE method identifies candidate key600 words by application of a statistical filter. The only input this method uses
is a stopwords list curated by the user. The statistical filter is based on the
computation of σ-index proposed by Ortuno et al. [32]. The idea is based on
the hypothesis that the spatial distribution of a word is prime determinant of
its relevance, irrespective of its frequency. The relevance is quantified by mea605 suring the standard deviation of the distance between successive occurrences of
the word in the text [32]. The use of this filter substantially reduces the search
space, and imparts language independence to this stage.
We consider Zhou et al. [48] for implementation of σ-index, which considers
boundary values for computation. Consider a word w that occurs n times in a
document of length N. Let pi denote the position of i
th occurrence of w, with
boundary values p0 and pN+1 set to 0 and N + 1, respectively. Then (pi+1 −pi)
denotes the distance between two consecutive occurrences of w. The average
distance between occurrences of w is given by µ(w),
µ(w) = (p1 − p0) + (p2 − p1) + ... + (pN+1 − pn)
n + 1
=
N + 1
n + 1
,
and the standard deviation is given by
s(w) =
vuut
1
n − 1
Xn
i=0
((pi+1 − pi) − µ(w))2
The σ-index σ(w) of w, is defined as
σ(w) = s(w)
µ(w)
, (6)
Table 11 shows the comparative performance of POS tag-based filter and
statistical filter. The values in each cell present the overlap between the gold610 standard keywords and the set of candidates obtained by each of these filters. It
30
ACCEPTED MANUSCRIPT
ACCEPTED MANUSCRIPT
is evident from the table that the best candidate list is obtained after performing
stemming on the list of nouns and adjectives. σ-index produces noisy candidates
list, which is the price paid for language independence.
Dataset σ PoST PoST+stem
Hulth2003 - 85.38 87.33
NLM500 53.12 69.54 76.90
Krapivin2009 88.13 92.94 95.45
SemEval2010 80.55 89.93 94.45
Table 11: Percentage overlap of goldstandard keywords and candidate lists obtained by: σ: σ-index, PoST: retaining
nouns and adjectives using POS tagging,
stem: with stemming enabled.
In order to find the threshold σ-index for retaining candidate keywords, we
615 inspected the ranks of gold-standard keywords by σ-index scores. Rugplots in
Figure 6 show higher density in the region corresponding to higher ranks (lower
values correspond to higher ranks). We found that on an average, more than 92%
gold-standard keywords out of those occurring explicitly in the text15 occur in
the top 33% words ranked using σ-index. Based on this observation, we decide
620 to retain top-33% candidates ranked based on σ-index. Since σ-index is not
suitable for small length documents, we do not apply this filter for Hulth2003
dataset in the experiments reported in Section 8.2.
Figure 6: Distribution of normalized σ-index ranks of gold-standard keywords contained in
the candidate lists
It is evident from Figure 6 that gold-standard keywords are usually ranked
15Some gold-standard keywords do not appear in the text.
31
ACCEPTED MANUSCRIPT
ACCEPTED MANUSCRIPT
higher based on their corresponding σ-index. For NLM500 dataset, the σ-index
625 based ranks of gold-standard keywords tend to gather towards top-33% with
anomalies lying towards lower ranks. This affects the performance for NLM500
dataset, which is reflected in the empirical results.
8.2. Experimental Evaluation: LAKE vs. sCAKE
We compare the performance of LAKE with sCAKE to assess the amount of
performance degradation due to non-adoption of NLP tools in LAKE method.
Table 12: Performance evaluation of LAKE vs. sCAKE
Datasets
sCAKE LAKE Performance
P R F1 P R F1 Loss in F1 (%)
Hulth2003 45.41 66.81 51.09 41.67 59.31 46.14 9.69
Krapivin2009 42.48 48.78 43.52 37.60 41.56 37.69 13.4
NLM500 24.88 34.99 28.29 19.60 26.55 21.87 22.69
Semeval2010 35.82 47.37 40.14 29.48 36.48 32.08 20.08
630
It is evident from the above table (Table 12) that there is a considerable performance gap between NLP-enabled and language-agnostic variation.
For English-like languages that enjoy the support of sophisticated NLP tools,
sCAKE is a better choice as it outperforms the other state-of-the-art keyword
635 extraction methods. However, for languages that lack the support of sophisticated NLP tools, there is no alternative approach provided by the existing
methods to enable language-independence feature. Thus, LAKE seems to be
a fair solution which can be applied on languages without linguistic support,
albeit with an associated cost of performance degradation.
640 8.3. Comparative Evaluation of Competing Methods
Figures 7(a-d) show comparative line graphs for sCAKE, LAKE, PositionRank, and K-core methods per dataset. It is evident that sCAKE (red opaque
diamond line) outperforms all other methods. Performance of LAKE is at par
with PositionRank, outperforming K-core in all four datasets. F1-score for K645 core does not improve with increasing keywords because K-core always extracts
32
ACCEPTED MANUSCRIPT
ACCEPTED MANUSCRIPT
words belonging to the top-most core as keywords. As stated earlier, F1-score
for all methods drop for very low and very high number of keywords. This is
because for less number of keywords, precision is usually high but recall is low.
On the other hand, for very large number of keywords, recall is high but pre650 cision is low. This ultimately affects the F1-score, bringing it down to a lower
value.
(a) Hulth2003 dataset (b) Krapivin2009 dataset
(c) NLM500 dataset (d) Semeval2010 dataset
Figure 7: Lineplot of F1-score for sCAKE, LAKE, PositionRank, and k-core on each dataset.
8.4. Experimentation on Indian Languages
India is a country with 23 official languages, including English. According
to Census of India of 2001, India has 122 major languages and 1599 other
655 languages. With such a wide variety of written and spoken languages, there is a
huge collection of literature available. However, due to scarcity of sophisticated
NLP tool, automatic analysis of such documents is challenging.
33
ACCEPTED MANUSCRIPT
ACCEPTED MANUSCRIPT
We evaluated LAKE method for automatic keyword extraction from an
Wikipedia article on ‘Animation’ written in Assamese. We removed English
660 characters from this document as an additional pre-processing step. The stopwords list used for this exercise is downloaded from TDIL website16. Top-10
extracted keywords, with their respective translations to English, are shown in
Table 13.
Table 13: Sample keywords extracted from Assamese text
The document along with the set of programs and stopwords list are available
at GitHub17
665 . Due to non-availability of gold-standard keywords set, we could
not evaluate the performance of LAKE on Assamese text. We leave it to the
readers to judge the performance based on the extracted keywords.
9. Conclusion
We present a commentary on graph-based keyword extraction methods, and
670 propose two new parameter-free methods sCAKE and LAKE. The two methods
are based on novel sentence-based graph construction approach (CAG) that
is mindful of the carriage of pragmatics from each sentence to its following
one. The novel word scoring approach (SCScore) computes the relevance of
words by taking into account its contextual hierarchy, semantic connectivity,
675 and positional weight in the text.
16https://www.tdil-dc.in/index.php?option=com_download&task=
showresourceDetails&toolid=1634&lang=en
17https://github.com/SDuari/LAKE-on-Assamese-text
34
ACCEPTED MANUSCRIPT
ACCEPTED MANUSCRIPT
We first evaluate the proposed graph construction and word scoring methods individually, and subsequently integrate as sCAKE algorithm. Four state-ofthe-art keyword extraction methods - TextRank, DegExt, k-core Retention, and
PositionRank were compared using four benchmark datasets. Experimental re680 sults reveal that the native word scoring methods perform better on CAG graphs
compared to the corresponding graphs. We also observe that the proposed word
scoring method performs consistently better than other scoring methods irrespective of the graph construction approach. Further, we show that the proposed keyword extraction method sCAKE outperforms PositionRank in terms
685 of F1-score.
A language-agnostic variant of sCAKE (called LAKE) is proposed which
employs statistical filter to identify candidate keywords. As expected, LAKE
suffers performance degradation compared to sCAKE on the studied datasets,
all of which consists of English texts. We conclude that for languages with so690 phisticated NLP support, it is better to exploit the linguistic features. However,
LAKE method can be applied on languages that are not supported with sophisticated NLP tools, albeit with an associated cost of performance degradation.
Top-10 keywords extracted (after stemming) by sCAKE method from this
manuscript18 are - “keyword”, “scake”, “extract”, “semant”, “connect”, “method”,
695 “text”, “awar”, “graph”, and “word”. All the words in the title are included in
the top-10 keywords list, which is desirable.
In future, we intend to apply LAKE on documents written in Indian Languages to see how well it performs on multiple languages and domains. We
also intend to make LAKE a benchmark, on the basis of which future keyword
700 extraction algorithms for Indian languages could be tested upon.
18Excluding conclusion, references, and other non-text entities like tables and figures with
captions
35
ACCEPTED MANUSCRIPT
ACCEPTED MANUSCRIPT
Acknowledgement
The authors acknowledge the financial support (Grant number RC/2015/9677)
awarded by University of Delhi, India for this research.
References
705 References
[1] Aronson, et al., 2000. The NLM Indexing Initiative, in: Proceedings of the
AMIA Symposium, American Medical Informatics Association. p. 17.
[2] Blanco, R., Lioma, C., 2012. Graph-based Term Weighting for Information
Retrieval. Information Retrieval 15, 54–92.
710 [3] Bookstein, A., Swanson, D.R., 1974. Probabilistic Models for Automatic
Indexing. JAIST 25, 312–316.
[4] Boudin, F., 2013. A Comparison of Centrality Measures for Graph-based
Keyphrase Extraction, in: International Joint Conference on Natural Language Processing (IJCNLP), pp. 834–838.
715 [5] Boudin, F., 2018. Unsupervised keyphrase extraction with multipartite
graphs, in: Proceedings of NAACL-HLT, pp. 667–672.
[6] Brin, S., Page, L., 1998. The Anatomy of a Large-scale Hypertextual Web
Search Engine. Computer Networks and ISDN Systems 30, 107–117.
[7] Carpena, P., Bernaola-Galv´an, P., Hackenberg, M., Coronado, A., Oliver,
720 J., 2009. Level Statistics of Words: Finding Keywords in Literary Texts
and Symbolic Sequences. Physical Review E 79, 035102.
[8] Carretero-Campos, C., et al., 2013. Improving Statistical Keyword Detection in Short Texts: Entropic and Clustering Approaches. Physica A:
Statistical Mechanics and its Applications 392, 1481–1492.
36
ACCEPTED MANUSCRIPT
ACCEPTED MANUSCRIPT
725 [9] Cohen, J., 2008. Trusses: Cohesive Subgraphs for Social Network Analysis.
National Security Agency Technical Report , 16.
[10] Dostal, M., Jeˇzek, K., 2011. Automatic Keyphrase Extraction based on
NLP and Statistical Method, in: Proceedings of the Dateso 2011, VSB - ˇ
Technical University of Ostrava. pp. 140–145.
730 [11] Ercan, G., Cicekli, I., 2007. Using Lexical Chains for Keyword Extraction.
Information Processing & Management 43, 1705–1714.
[12] Erkan, G., Radev, D.R., 2004. Lexrank: Graph-based Lexical Centrality
as Salience in Text Summarization. JAIR 22, 457–479.
[13] Florescu, C., Caragea, C., 2017. A Position-Biased PageRank Algorithm
735 for Keyphrase Extraction, in: AAAI, pp. 4923–4924.
[14] Frank, E., et al., 1999. Domain-specific Keyphrase Extraction, in: 16th International Joint Conference on Artificial Intelligence (IJCAI 99), Morgan
Kaufmann Publishers Inc., San Francisco, CA, USA. pp. 668–673.
[15] Grineva, M., Grinev, M., Lizorkin, D., 2009. Extracting Key Terms from
740 Noisy and Multitheme Documents, in: Proceedings of the 18th Tnternational Conference on World Wide Web, ACM. pp. 661–670.
[16] Harter, S.P., 1974. A Probabilistic Approach to Automatic Keyword Indexing. Ph.D. thesis. University of Chicago.
[17] Harter, S.P., 1975. A Probabilistic Approach to Automatic Keyword In745 dexing. Part II. An Algorithm for Probabilistic Indexing. Journal of the
Association for Information Science and Technology 26, 280–289.
[18] Herrera, J.P., Pury, P.A., 2008. Statistical Keyword Detection in Literary
Corpora. The European Physical Journal B 63, 135–146.
[19] Hulth, A., 2003. Improved Automatic Keyword Extraction given more
750 Linguistic Knowledge, in: Proceedings of the 2003 Conference on EMNLP,
ACL. pp. 216–223.
37
ACCEPTED MANUSCRIPT
ACCEPTED MANUSCRIPT
[20] Jaccard, P., 1901. Etude comparative de la distribution florale dans une ´
portion des alpes et des jura. Bull Soc Vaudoise Sci Nat 37, 547–579.
[21] Kaur, S., Saxena, R., Bhatnagar, V., 2017. Leveraging Hierarchy and
755 Community Structure for Determining Influencers in Networks, in: International Conference on DaWaK, Springer. pp. 383–390.
[22] Kim, S.N., Medelyan, O., Kan, M.Y., Baldwin, T., 2010. Semeval-2010
Task 5: Automatic Keyphrase Extraction from Scientific Articles, in: Proceedings of the 5th International Workshop on SemEval, ACL. pp. 21–26.
760 [23] Krapivin, M., Autaeu, A., Marchese, M., 2009. Large Dataset for
Keyphrases Extraction. Technical Report DISI-09-055 .
[24] Lahiri, S., Choudhury, S.R., Caragea, C., 2014. Keyword and Keyphrase
Extraction using Centrality Measures on Collocation Networks. arXiv
preprint arXiv:1401.6571 .
765 [25] Litvak, M., Last, M., Aizenman, H., Gobits, I., Kandel, A., 2011. DegExtA
Language-independent Graph-based Keyphrase Extractor, in: Advances in
Intelligent Web Mastering–3. Springer, pp. 121–130.
[26] Liu, Z., Huang, W., Zheng, Y., Sun, M., 2010. Automatic Keyphrase Extraction via Topic Decomposition, in: Proceedings of the 2010 Conference
770 on EMNLP, Association for Computational Linguistics. pp. 366–376.
[27] Luhn, H.P., 1957. A Statistical Approach to Mechanized Encoding and
Searching of Literary Information. IBM Journal of RD 1, 309–317.
[28] Manning, C.D., Sch¨utze, H., Raghavan, P., 2008. Introduction to information retrieval. Cambridge University Press.
775 [29] Matsuo, Y., et al., 2001. Keyworld: Extracting Keywords from Document’s
Small World, in: Discovery Science, Springer. pp. 271–281.
[30] Mihalcea, R., Tarau, P., 2004. TextRank: Bringing Order into Texts, in:
Proceedings of the 2004 Conference on EMNLP, ACL. pp. 404–411.
38
ACCEPTED MANUSCRIPT
ACCEPTED MANUSCRIPT
[31] Ohsawa, Y., Benson, N.E., Yachida, M., 1998. KeyGraph: Automatic In780 dexing by Co-occurrence Graph based on Building Construction Metaphor,
in: Proceedings of IEEE ADL, pp. 12–18.
[32] Ortuno, M., Carpena, P., Bernaola-Galv´an, P., Mu˜noz, E., Somoza, A.,
2002. Keyword Detection in Natural Languages and DNA. EPL (Europhysics Letters) 57, 759.
785 [33] Rousseau, F., Vazirgiannis, M., 2015. Main Core Retention on Graph-ofWords for Single-Document Keyword Extraction, in: Advances in Information Retrieval. Springer, pp. 382–393.
[34] Salton, G., Buckley, C., 1991. Automatic Text Structuring and RetrievalExperiments in Automatic Encyclopedia Searching, in: Proceedings of the
790 International ACM SIGIR Conference on Research and Development in
Information Retrieval, ACM. pp. 21–30.
[35] Savova, G.K., Masanz, J.J., Ogren, P.V., Zheng, J., Sohn, S., KipperSchuler, K.C., Chute, C.G., 2010. Mayo clinical Text Analysis and Knowledge Extraction System (cTAKES): Architecture, Component Evaluation
795 and Applications. Journal of the AMIA 17, 507–513.
[36] Seidman, S.B., 1983. Network Structure and Minimum Degree. Social
Networks 5, 269–287.
[37] Sparck Jones, K., 1972. A Statistical Interpretation of Term Specificity and
its Application in Retrieval. Journal of Documentation 28, 11–21.
800 [38] Tixier, A., Malliaros, F., Vazirgiannis, M., 2016. A graph degeneracy-based
approach to keyword extraction, in: Proceedings of the 2016 Conference
on Empirical Methods in Natural Language Processing, pp. 1860–1870.
[39] Turney, P.D., 2000. Learning Algorithms for Keyphrase Extraction. Information Retrieval 2, 303–336.
39
ACCEPTED MANUSCRIPT
ACCEPTED MANUSCRIPT
805 [40] Wang, M., Wang, C., Yu, J.X., Zhang, J., 2015. Community Detection
in Social Networks: An in-depth Benchmarking Study with a Procedureoriented Framework. Proceedings of the VLDB Endowment 8, 998–1009.
[41] Wasserman, S., Faust, K., 1994. Social network analysis: Methods and
applications. volume 8. Cambridge university press.
810 [42] Witten, I.H., Paynter, G.W., Frank, E., Gutwin, C., Nevill-Manning, C.G.,
1999. KEA: Practical Automatic Keyphrase Extraction, in: Proceedings
of the Fourth ACM Conference on Digital Libraries, ACM. pp. 254–255.
[43] You, W., Fontaine, D., Barth`es, J.P., 2013. An Automatic Keyphrase
Extraction System for Scientific Documents. KAIS 34, 691.
815 [44] Yu, X., Yu, Z., Liu, Y., Shi, H., 2017. Ci-rank: collective importance
ranking for keyword search in databases. Information Sciences 384, 1–20.
[45] Zaki, M.J., Meira Jr, W., Meira, W., 2014. Data mining and analysis:
fundamental concepts and algorithms. Cambridge University Press.
[46] Zhang, K., et al., 2006. Keyword extraction using support vector machine,
820 in: Advances in Web-Age Information Management, WAIM 2006, Lecture
Notes in Computer Science. Springer, Berlin, Heidelberg. pp. 85–96.
[47] Zhang, Y., Milios, E., Zincir-Heywood, N., 2007. A Comparative Study on
Key Phrase Extraction Methods in Automatic Web Site Summarization.
Journal of Digital Information Management 5, 323.
825 [48] Zhou, H., Slater, G.W., 2003. A Metric to Search for Relevant Words.
Physica A: Statistical Mechanics and its Applications 329, 309–327.
40